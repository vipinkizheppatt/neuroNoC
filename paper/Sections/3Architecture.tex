\section{Proposed Architecture}

\begin{figure}
    \centering
    \includegraphics[width =0.8\columnwidth]{Figures/NeuroNoC.pdf}
    \caption{Architecture of the proposed artificial neuron}
    \label{fig:neuronoc}
\end{figure}

The overall architecture of NeuroNoC is depicted in Fig.~\ref{fig:neuronoc}.
The NoC is arranged in mech topology with each PE representing a neuron, except the one at the bottom-left corner.
Mesh topology was selected based on its higher bisection bandwidth and suitability in implementing multicast systems. 
The size of the NoC is configurable through an HDL parameter and can be set by users based on the type of the application and the resource capacity of the target FPGA device.
The bottom-left PE (with zero X and Y coordinates) is used to inject data to the network from external world.
In other words this PE represents the entire input layer of the NN.
The output layer of the NN sends back the network output to this PE for transmitting it back to the host machine.
Due to the packet-switched nature of NoC architecture, the packets may reach the output PE in out-of-order.
Packets are reordered and sent back to the host computer based on the sequence number embedded in the data packets.

\subsection{Packet Formats}
\label{subsecpktformat}
NeuroNoC implements a variety of packet formats for supporting network and neuron configurations as well as for inter-neuron communication.  
Fig.~\ref{figure:pktformat} depicts the different packet types and their corresponding fields.
For a given NeuroNoC implementation, all packet types have same size but depending on the size of the NoC, packet size varies.
This is attributed to the variation in the size of the address fields which are NoC size dependent. 
\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=0.8\columnwidth]{Figures/pktformat.pdf}
    \caption{NeuroNoC packet formats for supporting neuron and network configurations and data communication} 
    \label{figure:pktformat}
    \end{center}
\end{figure}

\subsubsection*{\bf Data Packet}
Data packets are used for inter-neuron communication.
Once a neuron receives data packets from all its predecessors, it calculates the output based on inputs values, weights, bias and the activation function.
It sends out another data packet with the calculated output in the \emph{Data} field and the neuron address in the \emph{source} field.
The index field has significance only when the packet source is zero (bottom-left PE).
Since the entire input layer is modeled by this single PE, the index number differentiates the different neurons of this layer.
Otherwise it will be impossible to determine the weight values corresponding each input neuron in the first hidden layer.
\subsubsection*{\bf Input number and Bias Configuration Packet}
This configuration packet is used for setting two parameters in each neuron.
The \emph{Input Number} field configures the number of predecessors from the previous layer.
The \emph{Bias} field sets the neuron bias for calculating its output.
Each neuron stores these two parameters in appropriate registers during network configuration.
The \emph{Destination} field is used by the NoC to route the packet to appropriate neurons. 
\subsubsection*{\bf Weight Configuration Packet}
The weight configuration packet sets the input weights for each neuron by storing the \emph{Weight} field in its internal Weight Table.
The \emph{Source} field enables mapping a a particular weight with a particular input.
Infact the Source field is used as the write address when storing weight in the weight table.
\subsubsection*{\bf Routing Table Configuration Packet}
These packets are used for configuring the NoC switch routing tables, enabling multicast routing algorithm described in subsection~\ref{subsec_routing}.


\begin{figure}
    \centering
    \includegraphics[width =0.8\columnwidth]{Figures/neuron.pdf}
    \caption{Architecture of an artificial neuron}
    \label{fig:neuron}
\end{figure}

\subsection{Network Initialization}
The following steps have to be performed by an external host machine before sending training or test data to the network.
\begin{itemize}
\item Initialize the routing tables of the NoC by sending \emph{forwarding table configuration} packets
\item Initialize each neuron with its number of predecessors and the bias value through \emph{input number and bias configuration} packets
\item Configure the weight tables of the neurons with either pre-trained weight values or random initialization values through the \emph{weight configuration} packets.
\end{itemize}
Presently NeuroNoC does not support on-chip training of the network.
This is mainly because the training algorithms such as gradient descent is not hardware friendly and even the approximate implementation consumes too much of hardware resources leaving very little for the actual network~\cite{}.
Hence either the network has to be pre-trained in a software environment to determine the weight values or a hardware-software co-design approach has to be adopted.
In the hardware-software approach, the weight tables are initialized with random weights and training samples are injected to the network by the host machine.
The network sends back the output to the host and it runs the back-propagation algorithm to determine the new weight values and updates it through configuration packets.
The same procedure is followed in determining the bias values also.
\subsection{Artificial Neuron}
Artificial neurons in the NN are implementing by the processing elements~(PEs) of the NoC.
In the present implementation, we support traditional artificial neurons with linear, sigmoid, binary step and hyperbolic activation functions.
In the future versions, more advanced neurons such as LSTM, spiking cells and recurrent cells will be supported.
The main advantage of the the proposed architecture is that, the NoC portion of the network can remain intact for inter-neuron communication and the processing portion of the network can be modified based on requirements.
Each neuron receives inputs from neurons in the previous layer, multiplies them with corresponding weights, sums up and generates the output based on an activation function.
During configuration stage, each neuron is configured with the numbers, weights corresponding each input and bias.
Fig.~\ref{fig:neuron} shows the different submodules of an artificial neuron and are described in the subsequest sections.
\subsubsection{\bf Sequence Number Checker (SNC)}
 
Since packet switched NoCs do not preserve packet delivery order, neurons support out-of-order packet delivery by storing and reassembling the incoming data packets. 
The input layer (bottom-left PE) assigns each data packet a sequence number in the \emph{sequence number} field. 
It is to be noted that the number assigned corresponds to an input number rather than to a single packet.
In other words, an input with \emph{n-feature vector space} will be sent as n packets with the same sequence number. 
For example when the NN is used for detecting hand written characters, the input to the network are grayscale images.
Here each input is a grayscale image, which is composed of 784 pixels.
Thus each input to the network is a 784-feature vector, where each feature is an intensity value of a pixel.
A single input will be injected to the network as 784 packets with the same sequence number.
It should be also noted that for this network the number of neurons in the input layer are 784, but in the NoC implementation they are represented as 784 packets with same sequence number.
To distinguish each input neuron, the packets also carry a sequence number field as discussed in the previous section.
In the absence of this field, the first hidden layer will not be able to determine the corresponding weight values for each input neuron.
The index has no significance except for the first hidden layer since every neuron is represented by a physical PE except for the input layer.
After processing the input data, the output packet from a neuron retains the same sequence number as that of the input packets. 

The neuron initially stores the received data packets in a hash table~(HT). 
The table consists of $2^{SEQ\_WIDTH}$ blocks, where $SEQ\_WIDTH$ is the number of bits in the seqNum field. 
For a given NoC of size $S_{NOC}$, in the worst case each neuron may receive inputs from $S_{NOC}$ neighboring neurons. 
Thus, every block in HT is further divided to $S_{NOC}$ memory units. 
The data packets are mapped to HT memory through the following hash function,


\begin{equation}
\resizebox{.9\hsize}{!}{\emph{hf(seqNum,count[seqNum]) = concat(seqNum,count[seqNum])}}
\label{equation:hf}
\end{equation}

Where \emph{count[seqNum]} is the number of packets with the same sequence number already received by the neuron.
The sequence number count is tracked by the counter array (CR), which has a dedicated counter for each sequence number. 
After a packet is stored in the HT, the corresponding sequence number counter is incremented, so that the next packets with the same sequence number is stored in the next memory location. 
The SNC State Machine (SSM) keeps track of the current sequence number and the corresponding counter to retrieve packets from the HT once the neuron receives packets from all its predecessors. 

\subsubsection{\bf Multipy-Accumulate Unit (MAC)}
The total synaptic input to the neuron is implemented by successive multiplication and addition operations (MAC operations).
Once the SNC module detects that the neuron has received input packets from all its predecessors, it sends a control signal to the MAC module.
The MAC module then reads the inputs one-by-one from the hash table~(HT) and multiplies and accumulates using the corresponding weight values read from the weight table~(WT). 
Weights are stored in WT through the configuration packets described in Section~\ref{subsecpktformat}. 
Assuming the same format for neuron input and output data, the number of bits to represent weighted sum is

\begin{equation}
N_{s}=\lceil\log_{2}(S_{NOC}\cdot (2^{n_{w}-1})(2^{n_{z}-1})+2^{n_{b}-1}\cdot 2^{f_{z}+f_{w}+f_{b}})\rceil+1
\label{equation:Ns}
\end{equation}

\subsubsection{\bf Activation Function (AF)}
The ANN implementation supports a variety of activation functions with the help of look-up-tables~(LUTs).
By changing the contents of the LUT, the activation function can be modified.
LUT-based implementation of a complex and non-linear activation function, such as sigmoid function, may require a large memory depending on the desired precision.  
If we define $n_{s}$ as the most significant bits of $N_{s}$, increasing the value of $n_{s}$ improves the accuracy at the expense of memory size. 
The minimum value at which all the possible output values are present in the LUT is given by

\begin{equation}
n_{s}=i_{s}-\lceil\log_{2}(\frac{d_{z}(1)}{{f}'(0)})\rceil
\label{equation:ns}
\end{equation}

Moreover, if we consider sigmoid function, most of the entries located far from 0 are duplicated. 
Considering this, it is possible to reduce the size of LUT to store the values in the interval $[x_{high}, x_{low}]$, in where the expressions for $x_{min}$ $x_{max}$ are given by

\begin{equation}
x_{high}=d_{s}(\ln{2^{f_{z}-1}}), x_{low}=-x_{high}
\label{equation:interval}
\end{equation}

and the number of bits to address the LUT is 
$n_{LUT}=\lceil\log_{2}{(x_{high}-x_{low}+1)}\rceil$.
If the computed weighted sum falls within this interval, the output is taken from the LUT otherwise the output is assigned 1 or 0 based on whether the value is above $x_{high}$ or below $x_{low}$. 

The output from the MAC module is used as the address for the activation function LUT.
The output from the LUT is then concatenated with the address of the PE as well as the sequence number received from the input packets to generate the neuron output packet.
This packet is then injected to the NoC for broadcasting to the successor neurons.
Since the NoC switches support multi-cast routing, each PE has to inject a single packet irrespective of the number of destinations.
This helps in reducing the latency and improves the overall throughput of the neurons. 
\subsection{Switch}
The detailed architecture of the NeuroNoC switch is depicted in Fig.~\ref{fig_switch}. 
Each switch is capable of sending and receiving packets from five direction; North, South, East, West and the associated neuron. 
Separate FIFOs are present for receiving and transmitting packets from each direction.
The interfaces follow AXI4-Stream interface for inter-swich as well as neuron-switch communication.
Switches serve the FIFOs in a queue in a clockwise direction.  Flow control is implemented through AXI-stream control signal as shown if figure.  The $o\_valid$ wires are asserted whenever switch transmits the data for certain direction including PE. Similarly, whenever the data comes from other switches or PE the $i\_valid$ wire of incoming side of the switch is asserted. The switch asserts the $o\_ready$ signal whenever the FIFO has empty slot to accept the data. All FIFOs should hold the data on the bus until data is transmitted to all necessary FIFO. Each switch has registers for routing table, size of total size of NoC rows with 5 bits length. Switch has finite state machine controller, which routes the packet for certain directions depending on the packet type and refreshes the routing table. In one process time one packet can be transmitted to several FIFOs. Switches communicate with each other through FIFOs.  

\begin{figure}
    \begin{center}
    \includegraphics[width=0.95\columnwidth]{Figures/switch2.pdf}
    \caption{Switch Architecture} 
    \end{center}
    \label{fig_switch}
\end{figure}

\subsection{Packet Routing}
\label{subsec_routing}
 Switch receives the packet from one FIFO and routes the data for required FIFOs. Finite state machine controller has three main approaches for routing the packets depending on the type of incoming packet.
 For weight and bias configuration packet switch sends the packet using the number of switch in the NoC. Depending on the number of destination switch, the present switch can route the packet for four directions. If the destination switch is located to the above, below, to the left or the right, it sends to North,South, East, West FIFOs respectively. Once the packet came to the predefined switch, instantly it will be sent to the PE FIFO corresponding to that switch for configuring the weight and bias.   
For routing table configuration packets switches behave the same as for weight and bias configuration oriented switches. The one difference is, when the packet came to the destination switch, it will refresh one slot of routing table and switch will serve next FIFO. 

While previous packets can be sent only for one direction(unicast) from one switch, the data packets can be replicated and sent for all direction including PE in one process time (multicast). It was achieved by the logic of routing table. Each switch has routing table, which must be configured before usage. The rows in routing table is equal to the total amount of switches in NoC. The actual meaning of rows in routing table is address of source switch. Whenever data packet comes, switch checks the source provider address of this packet and takes the row, which number is equal to source address. Each row has 5 bits for the decision to sending the packet for certain direction.  Each bit corresponds to 5 directions: North, South, East, West and PE. If the bit is equal to 1 then the incoming packet must be sent to corresponding direction,conversely, if bit is equal to 0 then the packet must not be sent to that direction. The routing table configuration directly depends on the neural network model and configuration packets must be created manually or using other software. For reliability of data in multicasted transmission, switch waits until the data is sent for all necessary direction, only after that it checks next FIFO. For implementation of packet routing the finite state machine principle was used and it is shown in figure ~\ref{figure:fsm}.


